{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJD/DPSzUVASBwWdW2yWD6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Try 2. Classification ver.**\n",
        "\n",
        "*  Dataset: superGLUE\n",
        "*  Model: T5-base\n",
        "\n"
      ],
      "metadata": {
        "id": "lHYSAzjwf-R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "RMY9R_sgtcma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import AdamW, T5TokenizerFast, T5ForConditionalGeneration, DataProcessor\n",
        "\n",
        "import wandb\n",
        "# from dataset import SoftDataset, task_to_target_len, get_tasks_processor\n",
        "# from utils import compute_task_metrics\n"
      ],
      "metadata": {
        "id": "5VqToPsHkXP7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfyGsuete4vQ"
      },
      "outputs": [],
      "source": [
        "class Finetuner:\n",
        "    def __init__(self, args, device):\n",
        "        self.superglue_datasets = [ 'axb', 'axg', 'boolq', 'cb', 'copa', 'multirc', 'record',  'rte', 'wic', 'wsc', 'wsc.fixed' ] # 수정 완료\n",
        "\n",
        "        # 학습 정보를 args(argument) 에 저장한 상태로 pass - 실제 학습 시 argument 딕셔너리 생성 필요\n",
        "        self.args = args\n",
        "        self.task = args.task\n",
        "        self.data_dir = args.data_dir\n",
        "\n",
        "        self.lr = args.lr\n",
        "        self.exp = args.exp\n",
        "        self.weight_decay = args.weight_decay\n",
        "        self.eps = args.eps\n",
        "        self.batch_size = args.batch_size\n",
        "        self.seq_len = args.seq_len\n",
        "        self.device = device\n",
        "        self.num_prompts = args.num_prompts\n",
        "        self.use_prompt_token = args.use_prompt_token\n",
        "        self.model_name = args.model_name\n",
        "        self.tokenizer = T5TokenizerFast.from_pretrained(args.model_name)\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(args.model_name)\n",
        "        self.tasks_data_dict = self.get_data_loader_dict() # 하단에 함수 정의\n",
        "        self.model = self.model.to(device)\n",
        "\n",
        "    def get_optimizer(self, lr, weight_decay, eps):\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "\n",
        "            # parameters with weight decay\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"lr\": lr,\n",
        "            },\n",
        "\n",
        "            # parameters without weight decay\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"lr\": lr,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, eps=eps)\n",
        "        return optimizer\n",
        "\n",
        "    # 전처리 함수\n",
        "    def get_data_loader_dict(self):\n",
        "        data_loader_dict = {}\n",
        "\n",
        "        target_len = task_to_target_len[self.task]\n",
        "        Processor = get_tasks_processor(self.args.processor, self.args.task)\n",
        "\n",
        "        train_data = Processor().get_train_examples(self.data_dir)\n",
        "        data_loader_dict['train'] = DataLoader(\n",
        "            dataset=SoftDataset(train_data, self.tokenizer, self.num_prompts, self.seq_len, target_len, True),\n",
        "            batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        dev_data = Processor().get_dev_examples(self.data_dir)\n",
        "        data_loader_dict['val'] = DataLoader(\n",
        "            dataset=SoftDataset(dev_data, self.tokenizer, self.num_prompts, self.seq_len, target_len, False),\n",
        "            batch_size=self.batch_size)\n",
        "\n",
        "        test_data = Processor().get_test_examples(self.data_dir)\n",
        "        data_loader_dict['test'] = DataLoader(\n",
        "            dataset=SoftDataset(test_data, self.tokenizer, self.num_prompts, self.seq_len, target_len, False),\n",
        "            batch_size=self.batch_size)\n",
        "\n",
        "        return data_loader_dict\n",
        "\n",
        "    def finetune(self, epochs):\n",
        "        print('finetune')\n",
        "        print(f'task = {self.task}')\n",
        "        print(f'pad_id = {self.tokenizer.pad_token_id}')\n",
        "        model = self.model\n",
        "        optimizer = self.get_optimizer(lr=self.lr, weight_decay=self.weight_decay, eps=self.eps)\n",
        "        dataloader_train = self.tasks_data_dict['train']\n",
        "        dataloader_val = self.tasks_data_dict['val']\n",
        "        target_len = task_to_target_len[self.task]\n",
        "\n",
        "        generation_arguments = {\n",
        "            \"max_length\": target_len,\n",
        "        }\n",
        "\n",
        "        max_acc, max_f1_score, max_em = 0, 0, 0\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            logs = None\n",
        "            loss_values, ppl_loss_values = [], []\n",
        "            for i, batch in enumerate(tqdm(dataloader_train)):\n",
        "                batch = {k: batch[k].to(model.device) for k in batch}\n",
        "                y = batch[\"target_ids\"]\n",
        "                lm_labels = y[:, :].clone().detach()\n",
        "                lm_labels[y[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "                loss = model(\n",
        "                    input_ids=batch[\"source_ids\"],\n",
        "                    labels=lm_labels\n",
        "                ).loss\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                loss_values.append(loss.detach().cpu().numpy())\n",
        "                logs = {\n",
        "                    'epoch': epoch,\n",
        "                    'train_loss': np.mean(loss_values)\n",
        "                }\n",
        "                if self.args.use_wandb:\n",
        "                    wandb.log(logs)\n",
        "\n",
        "                del batch\n",
        "                del lm_labels\n",
        "                del loss\n",
        "\n",
        "            with torch.no_grad():\n",
        "                print(logs)\n",
        "                y_pred, y_true, loss_values, ppl_loss_values = [], [], [], []\n",
        "                model.eval()\n",
        "                for i, batch in enumerate(tqdm(dataloader_val)):\n",
        "                    batch = {k: batch[k].to(model.device) for k in batch}\n",
        "\n",
        "                    outputs = model.generate(\n",
        "                        input_ids=batch['source_ids'],\n",
        "                        **generation_arguments,\n",
        "                    )\n",
        "\n",
        "                    pred = [self.tokenizer.decode(token_ids=ids, skip_special_tokens=True) for ids in outputs]\n",
        "                    target = [self.tokenizer.decode(token_ids=ids, skip_special_tokens=True) for ids in batch['target_ids']]\n",
        "                    model.train()\n",
        "                    if i == 0:\n",
        "                        print(f'example predictions pred: {pred} target: {target}')\n",
        "\n",
        "                    y_pred += pred\n",
        "                    y_true += target\n",
        "\n",
        "                    lm_labels = batch[\"target_ids\"]\n",
        "                    lm_labels[lm_labels[:, :] == 0] = -100\n",
        "\n",
        "                    loss = model(\n",
        "                        input_ids=batch['source_ids'],\n",
        "                        labels=lm_labels\n",
        "                    ).loss\n",
        "\n",
        "                    loss_values.append(loss.detach().cpu().numpy())\n",
        "\n",
        "                    del lm_labels\n",
        "                    del loss\n",
        "\n",
        "                logs = compute_task_metrics(task=self.task,\n",
        "                                            y_pred=y_pred,\n",
        "                                            y_true=y_true,\n",
        "                                            val_dataset=dataloader_val.dataset)\n",
        "                save_json = False\n",
        "                if 'acc' in logs:\n",
        "                    if max_acc < logs['acc']:\n",
        "                        max_acc = logs['acc']\n",
        "                        if self.args.save_model:\n",
        "                            model.save_pretrained(f'{self.args.output_dir}/{self.args.exp}_task{self.args.task}_{self.args.model_name.replace(\"/\",\"_\")}.pth', from_pt=True)\n",
        "                        save_json = True\n",
        "                    logs['max_acc'] = float(max_acc)\n",
        "                if 'f1_score' in logs:\n",
        "                    if max_f1_score < logs['f1_score']:\n",
        "                        max_f1_score = logs['f1_score']\n",
        "                    logs['max_f1_score'] = float(max_f1_score)\n",
        "                if 'em' in logs:\n",
        "                    if max_em < logs['em']:\n",
        "                        max_em = logs['em']\n",
        "                        if self.args.save_model:\n",
        "                            model.save_pretrained(f'{self.args.output_dir}/{self.args.exp}_task{self.args.task}_{self.args.model_name.replace(\"/\",\"_\")}.pth', from_pt=True)\n",
        "                        save_json = True\n",
        "                    logs['max_em'] = float(max_em)\n",
        "\n",
        "                logs['epoch'] = epoch\n",
        "                logs['val_loss'] = float(np.mean(loss_values))\n",
        "\n",
        "                print(logs)\n",
        "                if self.args.use_wandb:\n",
        "                    wandb.log(logs)\n",
        "                if self.args.save_model and save_json:\n",
        "                    with open(f'{self.args.output_dir}/{self.args.exp}_task{self.args.task}_{self.args.model_name.replace(\"/\",\"_\")}.json', 'w') as outfile:\n",
        "                        logs['model_name'] = self.args.model_name\n",
        "                        logs['exp'] = self.args.exp\n",
        "                        logs['task'] = self.args.task\n",
        "                        logs['lr'] = self.args.lr\n",
        "                        json.dump(logs, outfile, indent=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **이하 레퍼런스 코드**"
      ],
      "metadata": {
        "id": "Dyzety-hCLTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "\n",
        "\n",
        "class T5Dataset:\n",
        "    def __init__(self, tokenizer, task):\n",
        "        \"\"\"Dataset class for T5 model experiments.\n",
        "        Args:\n",
        "            task (str): Name of the downstream task.\n",
        "            tokenizer (HuggingFace Tokenizer): T5 model tokenizer to use.\n",
        "        \"\"\"\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.glue_datasets = ['cola', 'sst2', 'mrpc', 'qqp', 'stsb', 'mnli', \\\n",
        "                              'mnli_mismatched', 'mnli_matched', 'qnli', 'rte', 'wnli', 'ax']\n",
        "        self.superglue_datasets = ['copa', 'boolq', 'wic', 'wsc', 'cb', 'record', 'multirc', 'rte_superglue', 'wsc_bool']\n",
        "\n",
        "        # Column keys used in the dataset\n",
        "        self.task_to_keys = {\n",
        "            \"cola\": (\"sentence\", None),\n",
        "            \"mnli\": (\"premise\", \"hypothesis\"),\n",
        "            \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
        "            \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
        "            #\"qnli\": (\"question\", \"sentence\"),\n",
        "            \"qnli\": (\"text1\", \"text2\"),\n",
        "            \"qqp\": (\"question1\", \"question2\"),\n",
        "            \"rte\": (\"sentence1\", \"sentence2\"),\n",
        "            \"sst2\": (\"sentence\", None),\n",
        "            \"stsb\": (\"sentence1\", \"sentence2\"),\n",
        "            \"wnli\": (\"sentence1\", \"sentence2\"),\n",
        "\n",
        "            \"boolq\": (\"passage\", \"question\"),\n",
        "            \"copa\": ('choice1', 'choice2', 'premise', 'question'),\n",
        "            \"wic\": (\"start1\", \"end1\", \"sentence1\", \"start2\", \"end2\", \"sentence2\", \"word\"),\n",
        "            \"wsc\": (\"span1_text\", \"span1_index\", \"span2_text\", \"span2_index\", \"text\"),\n",
        "            \"wsc_bool\": (\"span1_text\", \"span1_index\", \"span2_text\", \"span2_index\", \"text\"),\n",
        "            \"cb\": (\"premise\", \"hypothesis\"),\n",
        "            \"record\": (\"passage\", \"query\", \"entities\"),\n",
        "            \"multirc\": (\"question\", \"answer\", \"paragraph\"),\n",
        "            \"rte_superglue\": (\"premise\", \"hypothesis\"),\n",
        "\n",
        "            \"scicite\": (\"sectionName\", \"string\"),\n",
        "            \"imdb\": (\"text\", None),\n",
        "\n",
        "            \"ag_news\": (\"text\", None),\n",
        "            \"yelp_review_full\": (\"text\", None),\n",
        "            \"yahoo_answers_topics\": (\"question_content\", \"best_answer\"),\n",
        "            \"dbpedia_14\": (\"title\", \"content\"),\n",
        "\n",
        "            \"ag\": (\"content\", None),\n",
        "            \"yelp\": (\"content\", None),\n",
        "            \"yahoo\": (\"content\", None),\n",
        "            \"dbpedia\": (\"content\", None),\n",
        "            \"amazon\": (\"content\", None),\n",
        "        }\n",
        "\n",
        "        # Label text for T5 tasks\n",
        "        # (T5 has text-to-text format for text and labels)\n",
        "        self.task_to_labels = {\n",
        "            \"cola\": (\"not_acceptable\", \"acceptable\"),\n",
        "            \"mnli\": (\"entailment\", \"neutral\", \"contradiction\"),\n",
        "            \"mnli-mm\": (),\n",
        "            \"mrpc\": (\"not_equivalent\", \"equivalent\"),\n",
        "            \"qnli\": (\"entailment\", \"not_entailment\"),\n",
        "            \"qqp\": (\"not_duplicate\", \"duplicate\"),\n",
        "            \"rte\": (\"entailment\", \"not_entailment\"),\n",
        "            \"sst2\": (\"negative\", \"positive\"),\n",
        "            \"stsb\": (),\n",
        "            \"wnli\": (),\n",
        "\n",
        "            \"boolq\": (\"false\", \"true\"),\n",
        "            \"copa\": (\"false\", \"true\"),\n",
        "            \"wic\": (\"false\", \"true\"),\n",
        "            \"wsc_bool\": (\"false\", \"true\"),\n",
        "            \"cb\": (\"entailment\", \"contradiction\", \"neutral\"),\n",
        "            \"multirc\": (\"false\", \"true\"),\n",
        "            \"rte_superglue\": (\"entailment\", \"not_entailment\"),\n",
        "\n",
        "            \"scicite\": (),\n",
        "            \"imdb\": (\"negative\", \"positive\"),\n",
        "\n",
        "            \"ag_news\": (\"world\", \"sports\", \"business\", \"science\"),\n",
        "            \"yelp_review_full\": (\"terrible\", \"bad\", \"middle\", \"good\", \"wonderful\"),\n",
        "            \"yahoo_answers_topics\": (\"society and culture\", \"science\", \"health\", \"education and reference\",\n",
        "                                     \"computers and internet\", \"sports\", \"business\", \"entertainment and music\",\n",
        "                                     \"family and relationships\", \"politics and government\"),\n",
        "            \"dbpedia_14\": (\"company\", \"educationalinstitution\", \"artist\", \"athlete\", \"officeholder\",\n",
        "                           \"meanoftransportation\", \"building\", \"naturalplace\", \"village\", \"animal\",\n",
        "                           \"plant\", \"album\", \"film\", \"writtenwork\"),\n",
        "\n",
        "            \"ag\": (\"world\", \"sports\", \"business\", \"science\"),\n",
        "            \"yelp\": (\"terrible\", \"bad\", \"middle\", \"good\", \"wonderful\"),\n",
        "            \"yahoo\": (\"society and culture\", \"science\", \"health\", \"education and reference\",\n",
        "                      \"computers and internet\", \"sports\", \"business\", \"entertainment and music\",\n",
        "                      \"family and relationships\", \"politics and government\"),\n",
        "            \"dbpedia\": (\"company\", \"educationalinstitution\", \"artist\", \"athlete\", \"officeholder\",\n",
        "                        \"meanoftransportation\", \"building\", \"naturalplace\", \"village\", \"animal\",\n",
        "                        \"plant\", \"album\", \"film\", \"writtenwork\"),\n",
        "            \"amazon\": (\"terrible\", \"bad\", \"middle\", \"good\", \"wonderful\"),\n",
        "        }\n",
        "\n",
        "        self.task = task\n",
        "        self.label_key = 'label'\n",
        "        if 'yahoo_' in task: self.label_key = 'topic'\n",
        "        if 'stsb' in task: self.label_key = 'similarity_score'\n",
        "        if task=='record': self.label_key = 'answers'\n",
        "\n",
        "\n",
        "    # Helper function to save idx of multirc questions (needed later for test metric computation)\n",
        "    def save_multirc_questions_idx(self, val_ds):\n",
        "        idx = []\n",
        "        i = 0\n",
        "        x_prev, y_prev= val_ds['paragraph'][0], val_ds['question'][0]\n",
        "\n",
        "        for x,y in zip(val_ds['paragraph'], val_ds['question']):\n",
        "            if x_prev!=x or y_prev!=y:\n",
        "                i += 1\n",
        "            x_prev = x\n",
        "            y_prev = y\n",
        "            idx.append(i)\n",
        "        self.multirc_idx = np.array(idx)\n",
        "\n",
        "\n",
        "    # Helper function to select a subset of k samples per class in a dataset\n",
        "    def select_subset_ds(self, ds, k=2000, seed=0):\n",
        "        if self.task in ['stsb', 'record', 'wsc']: # non-discrete labels\n",
        "            idx_total = np.random.choice(np.arange(ds.shape[0]), min(k,ds.shape[0]), replace=False)\n",
        "\n",
        "        else:\n",
        "            label_key = self.label_key\n",
        "            N = len(ds[label_key])\n",
        "            idx_total = np.array([], dtype='int64')\n",
        "\n",
        "            for l in set(ds[label_key]):\n",
        "                idx = np.where(np.array(ds[label_key]) == l)[0]\n",
        "                idx_total = np.concatenate([idx_total, # we cannot take more samples than there are available\n",
        "                                            np.random.choice(idx, min(k, idx.shape[0]), replace=False)])\n",
        "\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(idx_total)\n",
        "        return ds.select(idx_total)\n",
        "\n",
        "\n",
        "    # WSC task function to preprocess raw input & label text into tokenized dictionary\n",
        "    def process_wsc(self, wsc_row):\n",
        "        text_proc = wsc_row['text'].split(' ')\n",
        "        #text_proc[wsc_row['span1_index']] = '*' + text_proc[wsc_row['span1_index']] +'*'\n",
        "        target = text_proc[wsc_row['span1_index']]\n",
        "        text_proc[wsc_row['span2_index']] = '*' + text_proc[wsc_row['span2_index']] + '*'\n",
        "        text_proc = (' ').join(text_proc)\n",
        "        return text_proc, target\n",
        "\n",
        "\n",
        "    # Function to preprocess raw input & label text into tokenized dictionary\n",
        "    def preprocess_function(self, examples, task,\n",
        "                            max_length=512, max_length_target=2,\n",
        "                            prefix_list=[]):\n",
        "        tokenizer = self.tokenizer\n",
        "        keys = self.task_to_keys[task]\n",
        "        label_key = self.label_key\n",
        "\n",
        "        if keys[1]!=None:\n",
        "            if task=='record':\n",
        "                text = 'passage : ' + str(examples['passage']) + ' query: ' + str(examples['query']) + ' entities: ' + ('; ').join((examples['entities']))\n",
        "            elif task=='wsc':\n",
        "                text, target = self.process_wsc(examples)\n",
        "            else:\n",
        "                text = ''\n",
        "                for key in keys:\n",
        "                    text += key + ': ' + str(examples[key]) + ' '\n",
        "        else:\n",
        "            text = examples[keys[0]]\n",
        "\n",
        "        if len(prefix_list)>0:\n",
        "            text = (' ').join(prefix_list) + ' ' + text\n",
        "        source = tokenizer(text.strip()+' </s>',\n",
        "                          truncation=True,\n",
        "                          #padding=False,\n",
        "                          padding='max_length',\n",
        "                          max_length=max_length)\n",
        "\n",
        "        if task=='stsb':\n",
        "            target = str(examples[label_key])[:3]\n",
        "        elif task=='record':\n",
        "            target = '; '.join(examples[label_key])\n",
        "        elif task=='wsc':\n",
        "            pass # already obtained target\n",
        "        else:\n",
        "            target = self.task_to_labels[task][examples[label_key]]\n",
        "        target += ' </s>'\n",
        "        target = tokenizer(\n",
        "                  target, max_length=max_length_target, pad_to_max_length=True, #return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "        dict_final = {\"source_ids\": source['input_ids'],\n",
        "                      \"source_mask\": source['attention_mask'],\n",
        "                      \"target_ids\": target['input_ids'],\n",
        "                      \"target_mask\": target['attention_mask']}\n",
        "        return dict_final\n",
        "\n",
        "\n",
        "\n",
        "    def get_final_ds(self,\n",
        "                     task,\n",
        "                     split,\n",
        "                     batch_size,\n",
        "                     k=-1,\n",
        "                     seed=0,\n",
        "                     return_test=False,\n",
        "                     target_len=2,\n",
        "                     max_length=512,\n",
        "                     prefix_list=[]):\n",
        "        \"\"\"Function that returns final T5 dataloader.\n",
        "        Args:\n",
        "            task (str): Name of the downstream task.\n",
        "            split (str): Which data split to use (train/validation/test).\n",
        "            batch_size (int): Batch size to use in the dataloader.\n",
        "            k (int, optional): Number of samples to use for each class. Defaults to -1, not sub-sample the data.\n",
        "            seed (int, optional): Seed used for random shuffle. Defaults to 0.\n",
        "            return_test (bool, optional): Whether to create a test split.\n",
        "                When True, two Dataloaders are returned. Defaults to False.\n",
        "            target_len (int, optional): Length of the model output (in tokens). Defaults to 2.\n",
        "            max_length (int, optional): Length of the model input (in tokens). Defaults to 512.\n",
        "            prefix_list (List[str], optional): List of prompt virtual tokens to pre-pend to the input.\n",
        "                We do not encode soft prompt as extra virtual tokens in the latest implementation.\n",
        "                Defaults to [], empty list.\n",
        "\n",
        "        Returns:\n",
        "            Dataloader: Torch Dataloader with preprocessed input text & label.\n",
        "        \"\"\"\n",
        "\n",
        "        if task in ['amazon']: # amazon not available with hugging face\n",
        "            df = pd.read_csv('../datasets/src/data/'+task+'/'+split+'.csv', header=None)\n",
        "            df = df.rename(columns={0: \"label\", 1: \"title\", 2: \"content\"})\n",
        "            df['label'] = df['label'] - 1\n",
        "            dataset = datasets.Dataset.from_pandas(df)\n",
        "        elif task == 'mnli':\n",
        "            dataset = load_dataset('LysandreJik/glue-mnli-train', split=split)\n",
        "        elif task == 'qnli':\n",
        "            dataset = load_dataset('SetFit/qnli', split=split)\n",
        "        elif task == 'stsb':\n",
        "            dataset = load_dataset('stsb_multi_mt', name='en', split=split if split=='train' else 'dev')\n",
        "        else:\n",
        "            if task not in self.glue_datasets and task not in self.superglue_datasets:\n",
        "                dataset = load_dataset(task, split=split)\n",
        "            else:\n",
        "                benchmark = 'glue' if task not in self.superglue_datasets else 'super_glue'\n",
        "                dataset = load_dataset(benchmark,\n",
        "                                       task.replace('_superglue', '').replace('_bool', ''),\n",
        "                                       split=split)\n",
        "\n",
        "        # For yahoo dataset we need to filter out empty rows\n",
        "        # (i.e. where \"question\" field is empty)\n",
        "        if self.task == \"yahoo_answers_topics\":\n",
        "            if split=='train':\n",
        "                good_id = np.load('good_id_yahoo_train.npy')\n",
        "                dataset = dataset.select(good_id)\n",
        "            elif split=='test':\n",
        "                good_id = np.load('good_id_yahoo_test.npy')\n",
        "                dataset = dataset.select(good_id)\n",
        "\n",
        "        # Using Lester et al. setting for WSC task, e.g.\n",
        "        # using only positive samples (for output generation)\n",
        "        if self.task == 'wsc':\n",
        "            idx = np.where(np.array(dataset['label']) == 1)[0]\n",
        "            dataset = dataset.select(idx)\n",
        "\n",
        "        # Selecting k subset of the samples (if requested)\n",
        "        if k!=-1:\n",
        "            dataset = self.select_subset_ds(dataset, k=k)\n",
        "\n",
        "        if k==-1 and split!='train' and self.task=='multirc':\n",
        "            # we do not shuffle full validation set of multirc\n",
        "            # but we save idx of the same questions\n",
        "            # which are used for multirc test metric computation\n",
        "            self.save_multirc_questions_idx(dataset)\n",
        "        else:\n",
        "            dataset = dataset.shuffle(seed=seed)\n",
        "\n",
        "        # Returning the selected data split (train/val/test)\n",
        "        if return_test==False:\n",
        "            encoded_dataset = dataset.map(lambda x: self.preprocess_function(x, task,\n",
        "                                                                            max_length=max_length,\n",
        "                                                                            max_length_target=target_len,\n",
        "                                                                            prefix_list=prefix_list),\n",
        "                                          batched=False)\n",
        "            encoded_dataset.set_format(type='torch', columns=['source_ids', 'source_mask',\n",
        "                                                              'target_ids', 'target_mask'])\n",
        "            dataloader = DataLoader(encoded_dataset, batch_size=batch_size)\n",
        "\n",
        "            return dataloader\n",
        "\n",
        "        # Creating an extra test set from the selected data split\n",
        "        else:\n",
        "            N = len(dataset)\n",
        "            dataset_val = dataset.select(np.arange(0, N//2))\n",
        "            dataset_test = dataset.select(np.arange(N//2, N))\n",
        "\n",
        "            dataloaders_val_test = []\n",
        "            for dataset in [dataset_val, dataset_test]:\n",
        "                encoded_dataset = dataset.map(lambda x: self.preprocess_function(x, task,\n",
        "                                                                                 max_length=max_length,\n",
        "                                                                                 max_length_target=target_len,\n",
        "                                                                                 prefix_list=prefix_list),\n",
        "                                              batched=False)\n",
        "                encoded_dataset.set_format(type='torch', columns=['source_ids', 'source_mask',\n",
        "                                                                  'target_ids', 'target_mask'])\n",
        "                dataloader = DataLoader(encoded_dataset, batch_size=batch_size)\n",
        "                dataloaders_val_test.append(dataloader)\n",
        "\n",
        "            return dataloaders_val_test"
      ],
      "metadata": {
        "id": "F1K6aA7wCIxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import logging, os, argparse\n",
        "\n",
        "from t5_continual import T5ContinualLearner\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    save_path = os.path.join(args.save_dir, args.save_name)\n",
        "    if not os.path.exists(save_path):\n",
        "        os.mkdir(save_path)\n",
        "    task_list = args.task_list\n",
        "\n",
        "    model_name = args.model_name\n",
        "    continual_learner = T5ContinualLearner(model_name,\n",
        "                                           task_list,\n",
        "                                           batch_size=args.batch_size,\n",
        "                                           select_k_per_class=args.select_k_per_class,\n",
        "                                           prefix_len=args.prefix_len,\n",
        "                                           freeze_weights=args.freeze_weights==1,\n",
        "                                           freeze_except=args.freeze_except,\n",
        "                                           lr=args.lr,\n",
        "                                           seq_len=args.seq_len,\n",
        "                                           early_stopping=args.early_stopping==1,\n",
        "                                           prefix_MLP=args.prefix_MLP,\n",
        "                                           prefix_path=args.prefix_path if args.prefix_path!='' else None,\n",
        "                                           mlp_layer_norm=args.mlp_layer_norm==1,\n",
        "                                           bottleneck_size=args.bottleneck_size,\n",
        "                                           get_test_subset=args.get_test_subset==1,\n",
        "                                           memory_perc=args.memory_perc\n",
        "                                           )\n",
        "    if args.get_test_subset==0:\n",
        "        print(\"Not creating test subset\")\n",
        "\n",
        "    if args.multitask == 1:\n",
        "        print('Multi task learning')\n",
        "        results_dict = continual_learner.multi_task_training(num_epochs=args.num_epochs, save_path=save_path)\n",
        "        np.save(os.path.join(save_path, 'results_dict.npy'), results_dict)\n",
        "\n",
        "    else:\n",
        "        if args.num_epochs<=50:\n",
        "            eval_every_N = 1\n",
        "        elif args.num_epochs>50 and args.num_epochs<=200:\n",
        "            eval_every_N = 5\n",
        "        elif args.num_epochs>200:\n",
        "            eval_every_N = 10\n",
        "\n",
        "        results_dict = continual_learner.train_continual(continual_learner.task_list,\n",
        "                                                        epochs=args.num_epochs,\n",
        "                                                        save_path=save_path,\n",
        "                                                        progressive=args.progressive==1,\n",
        "                                                        eval_every_N=eval_every_N,\n",
        "                                                        test_eval_after_every_task=args.test_eval_after_every_task==1,\n",
        "                                                        data_replay_freq=args.data_replay_freq,\n",
        "                                                        )\n",
        "        np.save(os.path.join(save_path, 'results_dict.npy'), results_dict)\n",
        "        np.save(os.path.join(save_path, 'prompts.npy'), continual_learner.previous_prompts.detach().cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(\n",
        "      description='NLP training script in PyTorch'\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--save_dir',\n",
        "        type=str,\n",
        "        help='base directory of all models / features (should not be changed)',\n",
        "        default='/data/home/arazdai/T5_prompts/T5_continual/' #'/scratch/hdd001/home/anastasia/CL/'\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--save_name',\n",
        "        type=str,\n",
        "        help='folder name to save',\n",
        "        required=True\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--task_list',\n",
        "        nargs='+',\n",
        "        help='List of tasks for training',\n",
        "        required=True\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--model_name',\n",
        "        type=str,\n",
        "        help='Name of the model used for training',\n",
        "        default=\"t5-base\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--num_epochs',\n",
        "        type=int,\n",
        "        help='Number of epochs to train model',\n",
        "        default=5\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--multitask',\n",
        "        type=int,\n",
        "        help='Whether to perform multi-task training',\n",
        "        default=0\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--batch_size',\n",
        "        type=int,\n",
        "        help='Batch size',\n",
        "        default=8\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--seq_len',\n",
        "        type=int,\n",
        "        help='Length of a single repeat (in #tokens)',\n",
        "        default=512\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--prefix_len',\n",
        "        type=int,\n",
        "        help='Length of prompt (in #tokens)',\n",
        "        default=10\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--prefix_path',\n",
        "        type=str,\n",
        "        help='path to a pre-trained progressive prefix (for superGLUE experiments)',\n",
        "        default=''\n",
        "    )\n",
        "\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--lr',\n",
        "        type=float,\n",
        "        help='Learning rate',\n",
        "        default=0.3\n",
        "    )\n",
        "\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--memory_perc',\n",
        "        type=float,\n",
        "        help='Memory perc',\n",
        "        default=0.01\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--data_replay_freq',\n",
        "        type=float,\n",
        "        help='Replay data every X iterations',\n",
        "        default=-1\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--select_k_per_class',\n",
        "        type=int,\n",
        "        help='Select k examples from each class (default -1, i.e. no changes to the original dataset)',\n",
        "        default=-1\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--test_eval_after_every_task',\n",
        "        type=int,\n",
        "        help='Whether to re-evaluate test accuracy after every task (0 - False, 1 - True)',\n",
        "        default=0\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--progressive',\n",
        "        type=int,\n",
        "        help='Whether to concatenate prompts in a progressive way (0 - False, 1 - True)',\n",
        "        default=1\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--freeze_weights',\n",
        "        type=int,\n",
        "        help='Whether to freeze model weigts (except word emb)',\n",
        "        default=0\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--freeze_except',\n",
        "        type=str,\n",
        "        help='If freeze_weights==1, freeze all weights except those that contain this keyword',\n",
        "        default='xxxxxxx' # freeze all\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--get_test_subset',\n",
        "        type=int,\n",
        "        help='Whether to create a separate test split',\n",
        "        default=1\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--early_stopping',\n",
        "        type=int,\n",
        "        help='If early_stopping==1, do early stopping based on val accuracy',\n",
        "        default=1 # freeze all\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--prefix_MLP',\n",
        "        type=str,\n",
        "        help='Type of MLP reparametrization (if None - use Lester original implementation)',\n",
        "        default='None' # freeze all\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--mlp_layer_norm',\n",
        "        type=int,\n",
        "        help='Do layer norm in MLP',\n",
        "        default=1 # use layer norm\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        '--bottleneck_size',\n",
        "        type=int,\n",
        "        help='MLP bottleneck size',\n",
        "        default=800\n",
        "    )\n",
        "\n",
        "    main(parser.parse_args())\n"
      ],
      "metadata": {
        "id": "H2zlY_xJCbk1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}